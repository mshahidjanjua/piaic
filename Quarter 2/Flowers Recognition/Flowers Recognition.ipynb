
import numpy as np
import pandas as pd 
from keras.models import Model
from keras import layers
from keras import Input

#1.Load data
#cardata = pd.read_csv("ionosphere_data.csv",sep=",") 
# Datasets from folders
daisydir = "flowers/daisy"
dandeliondir = "flowers/dandelion"
flowersdir = "flowers/flowers"
rosedir = "flowers/rose"
sunflowerdir = "flowers/c"
tulipdir = "flowers/tulip"
data = {
    'daisy':
    datasets.ImageFolder(root=daisydir, transform=image_transforms['daisy']),
    'dandelion':
    datasets.ImageFolder(root=dandeliondir, transform=image_transforms['dandelion']),
    'flowers':
    datasets.ImageFolder(root=flowersdir, transform=image_transforms['flowers']),
    'rose':
    datasets.ImageFolder(root=rosedir, transform=image_transforms['rose']),
    'sunflower':
    datasets.ImageFolder(root=sunflowerdir, transform=image_transforms['sunflower']),
    'tulip':
    datasets.ImageFolder(root=tulipdir, transform=image_transforms['tulip'])
}

#print (data['rose'])
# 3.Split into 60 and 40 ratio.
#Encode labels.
train, test  = np.split(data.sample(frac=1), [int(.6*len(data))])
y_train = np.array(train)
y_test = np.array(test)
#4.Model : 1 hidden layers including 16 unit.
text_vocabulary_size = 10000
question_vocabulary_size = 10000
answer_vocabulary_size = 500
num_samples = 1000
max_length = 100
text = np.random.randint(1, text_vocabulary_size,
size=(num_samples, max_length))
question = np.random.randint(1, question_vocabulary_size,size=(num_samples, max_length))
answers = np.random.randint(0, 1,size=(num_samples, answer_vocabulary_size))
model.fit([text, question], answers, epochs=10, batch_size=128)
model.fit({'text': text, 'question': question}, answers,
epochs=10, batch_size=128)
#5.Compilation Step (Note : Its a Binary problem , select loss , metrics according to it) 
#6.Train the Model with Epochs (100)
model.fit([text, question], answers, epochs=100, batch_size=128)
model.fit({'text': text, 'question': question}, answers,
epochs=10, batch_size=128)
#7.If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .

model = get_model()
model.train(train)

model = get_model()
model.test(test)
#8.Evaluation Step
#Prediction should be > 85%
UPPER_ALPHA = 0.86
upper_model = GradientBoostingRegressor(loss="quantile",
                                        alpha=UPPER_ALPHA)
test_score = upper_model.evaluate(test)
train_score = upper_model.evaluate(train)
#9 Prediction
preds = upper_model.predict(train)
print('Predicted:', decode_predictions(preds, top=3)[0])
